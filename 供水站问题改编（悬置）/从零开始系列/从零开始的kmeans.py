# 从零开始的kmeans

"""
K-means（K均值）算法是一种常用的聚类算法，其原理如下：

1. 初始化：选择要聚类的样本数据集，并选择聚类的数量K。随机选择K个样本作为初始聚类中心。

2. 分配：对于每个样本，计算其与每个聚类中心之间的距离（通常使用欧氏距离）。将样本分配给距离最近的聚类中心，形成K个初始聚类。

3. 更新：对于每个聚类，计算属于该聚类的样本的均值，作为新的聚类中心。

4. 重新分配和更新：重复步骤2和步骤3，直到满足终止条件。终止条件可以是达到最大迭代次数，或者聚类中心不再发生变化。

5. 输出结果：最终得到K个聚类，每个样本被分配到一个聚类中心。

K-means算法的目标是最小化样本与所属聚类中心之间的平方距离的总和（称为“簇内平方和”）。通过迭代的分配和更新步骤，不断优化聚类中心的位置，以使簇内平方和最小化。

K-means算法的特点：
- 是一种迭代算法，每次迭代都会改善聚类结果。
- 对于大型数据集效果良好，计算效率高。
- 对初始聚类中心的选择较为敏感，不同的初始选择可能会导致不同的聚类结果。
- 对离群点敏感，离群点可能会影响聚类结果。

需要注意的是，K-means算法是一种硬聚类方法，即每个样本只属于一个聚类中心。而对于一些模糊性较大的数据集，可以使用模糊C均值（Fuzzy C-means）算法。
"""

import numpy as np
import random


#  第一个函数传入两个点x1和x2，可以获得这两个点的距离
def get_distance(x1, x2):
    return np.sqrt(np.sum(np.square(x1 - x2)))


# 第二个函数传入你想要获得的中心点数量k，和你给定的样本数据date，返回随机生成的初始中心点
def center_init(k, date):
    # 后续用于储存样本的数量和特征
    n_samples, n_features = date.shape
    # 用于储存中心点
    centers = np.zeros((k, n_features))
    # 用于储存中心点的在样本数据中的索引
    selected_centers_index = []

    # 循环k次，每次随机取一个不重复的中心点
    for i in range(k):
        # 从所有样本的索引抽取不重复的索引作为中心点
        selected_index = random.choice(list(set(range(n_samples)) - set(selected_centers_index)))
        # 根据上一行随机抽出的中心点索引从全部数据中找到相应的中心点坐标，储存在centers里面
        centers[i] = date[selected_index]
        # 已经抽取过作为中心点的索引加入selected_centers_index，下次再抽取新的中心点索引时会把这个索引减去，防止重复抽取
        selected_centers_index.append(selected_index)

    return centers


# 接下来，我们要把所有样本点分类到上文所选出的中心点中，具体方法是判断该样本点与所有中心点的距离，它离哪个中心点最近，就分到哪个中心点
# 这个函数将完成这个分类的功能，传入你要判断的一个样本点坐标sample和所有中心点坐标centers，它将返回离样本点最近的那个中心点的索引
def closest_center(sample, centers):
    # 下面两行是初始化，closest_i用于储存最近中心点在centers中的索引，默认设为0；closest_distance用于储存样本点离最近中心点的距离，默认设为无穷
    closest_i = 0
    closest_distance = float('inf')

    # 接下来遍历所有中心点，计算离样本点的距离，并比较，结果是找到离样本点最近的中心点的索引。i遍历的是中心点的索引；c遍历的是中心点的坐标
    for i, c in enumerate(centers):
        # 此处用到了我们上面定义的get_distance（）函数，作用是获得两点之间的距离
        distance = get_distance(sample, c)
        if distance < closest_distance:
            # 如果我们目前遍历到的中心点距离小于之前所遍历的中心点距离，则将更新并记录这个中心点的索引和新的最近距离
            closest_i = i
            closest_distance = distance

    # 最后返回我们找到的，离给定的样本点最近的中心点的索引，相当于我们把给定的样本点归类到这个中心点之中了
    return closest_i


"""
看到这里已经很了不起了！你或许应该给自己一个大拇指！
现在我们稍微消化一下上面的内容，回顾一下我们都做了什么工作：
1.首先，我们定义了一个get_distance函数，他运用了两点间距离公式（勾股定理），让我们可以快速获得给定两个点的直线距离。注意，这里是直线距离，不等于下文我们要解决问题时的提到的管道长度。
2.接着，我们定义了一个center_init函数，只需传入我们想获得的中心点个数k和全部的坐标点信息date，它能从date中随机挑选k个中心点，并返回随机选出的所有中心点的坐标centers。
3.最后，我们定义了一个closest_center函数，我们的目的是给定一个样本点sample，判断它离哪个中心点最近，并返回这个中心点在centers中的索引。
进展巨大，我们几乎已经完成一半工作了！

接下来，看看还剩什么工作没做：
我们应该对每个date里面的样本点sample都执行一次closest_center。你可以把中心点（centers）想象成一个个小组长，我们根据座位距离的远近(两点之间距离)，把组员们（样本点sample）分到各个就近的小组长名下。
即分成k个组（因为有k个小组长），每个组有一个小组长（中心点）和若干组员（sample），我们把这样的一个组称为"一簇"
然后，我们要重新选举小组长（重新规划中心点），这次就不是随机选举了，而是要求找到每个组中离大家最近的最中心的那个组员作为新的小组长（新的中心点）
接着，我们按照新的小组长，重新对每个组员使用closest_center函数，给他们分配到新的小组中。以此类推，重复几次，目的是找到最适合的小组长和最适合的组员。

okk，让我们继续开始吧
"""


#  现在我们定义一个新的函数，它的功能是对每个date中的样本点（sample）都使用closest_center函数，把他们分给最近的中心点。
#  我们分别传入中心点centers、中心点数量k和全部点的坐标date，最后返回一个列表clusters，这个列表包含每个小组（簇）的组长（中心点）和组员（样本点）的信息（索引）
# cluster中文意思：群；组；簇
def create_cluster(centers, k, date):
    # 每个小组的相关数据都需要一个列表储存，我们有k个小组，因此需要k个空列表储存数据。再用把这七个列表包装到一个大列表里面，这就是我们的小组花名册了
    clusters = [[] for _ in range(k)]

    # 接下来给每个样本点分组。sample遍历的是样本点的坐标，sample_i遍历的是样本点在date中的索引
    for sample_i, sample in enumerate(date):
        # 找到离样本点最近的中心点的索引
        center_i = closest_center(sample, centers)
        # 把它分入那个组，并在那个组的列表中储存它在date中的索引
        clusters[center_i].append(sample_i)

    return clusters


# 接下来我们定义的函数将帮助我们重新选择小组长（中心点），注意，这之后中心点的选择都不是像第一次那样随机了
# 在这个函数中，我们传入上一个函数中获得的小组花名册clusters，中心点数量k和全部点的坐标date，返回通过计算获得的新的中心点centers
def calculate_new_centers(clusters, k, date):
    # 后续用于储存样本的数量和特征
    n_sample, n_features = date.shape
    # 用于储存中心点
    centers = np.zeros((k, n_features))
    # 你会发现以上两行代码和我们上文定义的center_init函数很像。没错！但下面我们就不再像center_init函数一样随机生成中心点了，而是找到每组点的最中心，作为新的中心点

    # 此处i遍历的是每组（每簇）的索引，而cluster遍历的是每组的小列表，里面包含组内每个点坐标
    for i, cluster in enumerate(clusters):
        # date[cluster] 表示从全部点坐标date中提取属于当前聚类cluster（里面储存了组员在date中的索引信息）的所有点的坐标，再用np.mean函数求平均。
        # axis=0 参数表示沿着每列计算平均值，也就是对每个特征进行求平均操作。综合上一条注释，我们知道这一行代码会返回一个具有每个特征的平均值的向量，即新的中心点。
        # 注意，这个新的中心点可能不存在于date中
        new_center = np.mean(date[cluster], axis=0)
        # 更新centers，用新的小组长new_center取代（覆写）旧的小组长
        centers[i] = new_center

    # 返回新的中心点坐标
    return centers


# 接下来，我们设计一个函数，展示分类结果，让我们看看这个kmeans算法效果究竟如何
# 这个代码需要传入小组花名册clusters和全部的点坐标数据date，返回分组信息y_pred，怎么说有点抽象，完成这个函数定义后，我会让gpt举个例子帮助理解
def get_cluster_labels(clusters, date):
    # 同样，还是先设置一个储存分组信息的矩阵y_pred
    # 其中，np.shape(X)[0] 表示获取元组中的第一个元素，即样本数量 n_samples
    y_pred = np.zeros(np.shape(date)[0])

    # 同样的， cluster_i遍历的是某小组在小组花名册中的索引，而cluster遍历的是每个在小组花名册中的小组
    for cluster_i, cluster in enumerate(clusters):
        # 接下来是遍历每个小组中组员sample在date中的索引，并储存到y_pred中
        for sample_i in cluster:
            y_pred[sample_i] = cluster_i
            print('把样本{}归到{}类'.format(sample_i, cluster_i))

    return y_pred


"""
对于get_cluster_labels函数，可能有点难以理解它的功能，下面我让gpt生成了一个例子供读者理解
当 get_cluster_labels 函数被调用，并传入以下参数值时：

clusters = [[0, 1], [2, 3], [4, 5]]
X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]
函数的输出结果将是一个长度为6的数组 y_pred，其中每个元素表示对应样本所属的聚类类别。

具体计算过程如下：

首先，通过 np.zeros(np.shape(X)[0]) 创建一个长度为6的全零数组 y_pred。
接着，对于每个聚类 cluster，按照聚类索引 cluster_i 进行遍历：
在第一个聚类 cluster=[0, 1] 中，样本索引为0和1。因此，将 y_pred[0] 和 y_pred[1] 的值分别设置为0，表示属于第0个聚类。
在第二个聚类 cluster=[2, 3] 中，样本索引为2和3。将 y_pred[2] 和 y_pred[3] 的值分别设置为1，表示属于第1个聚类。
在第三个聚类 cluster=[4, 5] 中，样本索引为4和5。将 y_pred[4] 和 y_pred[5] 的值分别设置为2，表示属于第2个聚类。
最终，返回结果为数组 y_pred = [0, 0, 1, 1, 2, 2]。其中，索引为0和1的样本被分配到第0个聚类，索引为2和3的样本被分配到第1个聚类，索引为4和5的样本被分配到第2个聚类。
因此，这个函数的输出就是描述每个样本所属的聚类类别的一个向量。你可以使用这个向量来进一步分析聚类结果、做可视化或者评估聚类性能。
"""


# 万事俱备，所有函数工具我们都已经集齐了，辛苦大家了！我们一鼓作气，正式组装kmeans算法吧！
# 我们需要传入全部点的坐标date，中心点的数量k和最大迭代次数max_iterations（也就是不断选新的小组长，分组，再选小组长，再分组的次数。数值越大，分组就越准确）
def kmeans(date, k, max_iterations):
    # 第一步，随机生成中心点
    centers = center_init(k, date)

    # 遍历迭代求解
    for _ in range(max_iterations):
        # 第二步，根据中心点分组，并生成小组花名册
        clusters = create_cluster(centers, k, date)
        # 存档当前的小组长，这点稍后有用
        pre_centers = centers
        # 第三步，重新选举小组长，得到新的中心点
        new_centers = calculate_new_centers(clusters, k, date)
        # 第四步，判断。假如新选举的小组长和之前的小组长没有区别，说明我们已经达到了迭代平衡点，就可以提前跳出循环，收工下班了
        difference = new_centers - pre_centers
        if difference.sum() == 0:
            # 如果新旧小组长没有区别，则提前跳出循环收工下班
            break

    # 第五步，返回最后的聚类标签
    return get_cluster_labels(clusters, date)


# 终于完成啦，下面给一个例子试一下
date = np.array([[26, 31], [5, 33], [8, 9], [10, 24], [20, 46], [17, 23], [20, 10], [25, 47], [31, 18], [35, 42],
               [36, 25], [41, 31], [45, 38], [41, 35], [40, 34], [38, 35], [38, 37], [33, 37], [31, 36], [33, 35],
               [28, 32], [24, 30], [21, 31], [22, 27], [28, 29], [43, 37], [44, 39], [25, 27], [21, 29], [22, 30],
               [24, 32], [37, 33], [38, 33], [37, 36], [14, 13], [16, 9], [14, 7], [18, 14], [12, 6], [15, 14],
               [20, 13], [13, 34], [16, 39], [21, 39], [26, 44], [28, 40], [27, 42], [29, 38], [29, 44], [36, 44],
               [41, 40], [39, 52], [27, 49], [23, 46], [13, 46], [16, 46], [22, 44], [40, 44], [42, 40], [37, 42],
               [35, 49], [35, 51], [35, 52], [34, 55], [26, 53], [27, 51], [31, 51], [31, 45], [31, 41], [28, 45],
               [27, 35], [24, 38], [26, 39], [13, 37], [17, 36], [21, 41], [18, 41], [21, 43], [13, 39], [14, 43],
               [12, 43], [10, 44], [16, 44], [18, 44], [24, 44], [25, 49], [24, 49], [24, 51], [21, 48], [17, 51],
               [10, 34], [9, 35], [7, 37], [4, 37], [5, 42], [2, 44], [7, 32], [7, 30], [1, 24], [2, 16], [3, 18],
               [2, 20], [4, 24], [5, 28], [6, 24], [9, 29], [2, 33], [7, 34], [3, 30], [3, 41], [10, 36], [17, 34],
               [20, 22], [24, 21], [22, 17], [21, 16], [27, 19], [26, 16], [9, 16], [12, 17], [14, 15], [19, 26],
               [14, 28], [13, 25], [9, 19], [2, 1], [6, 6], [7, 8], [6, 14], [5, 17], [5, 16], [16, 19], [26, 13],
               [29, 11], [31, 14], [28, 17], [20, 19], [17, 22], [15, 23], [21, 23], [24, 23], [26, 23], [25, 25],
               [15, 31], [15, 29], [10, 28], [38, 26], [37, 25], [33, 21], [40, 24], [44, 44], [41, 30], [33, 24],
               [32, 27], [40, 14], [42, 26], [45, 33], [29, 23], [31, 30], [30, 25], [31, 23], [35, 15], [40, 16],
               [40, 20], [37, 20], [35, 24], [43, 23], [45, 26], [37, 28], [35, 28], [33, 29], [37, 30], [39, 30],
               [41, 29], [43, 31], [47, 34], [46, 43], [42, 43], [48, 45], [42, 44], [43, 50]])

#kmean函数里，第一个传入的是全部点的坐标，第二个是你想获得的小组数量（中心点数量），第三个是最大迭代次数，次数越多越可能准确
print(kmeans(date, 13, 15))
